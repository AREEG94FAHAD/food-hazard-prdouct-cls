{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10587601,"sourceType":"datasetVersion","datasetId":6552505}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file\ndf = pd.read_csv('datatset - augem - path')\n\n# Display all columns\nprint(df.columns)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T18:38:34.681878Z","iopub.execute_input":"2025-01-26T18:38:34.682088Z","iopub.status.idle":"2025-01-26T18:38:36.023666Z","shell.execute_reply.started":"2025-01-26T18:38:34.682049Z","shell.execute_reply":"2025-01-26T18:38:36.022655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n\n# Load train and validation datasets\ntrain_data = pd.read_csv(\"datatset - augem - path.csv\")\nvalid_data = pd.read_csv(\"incidents_valid.csv\", index_col=0)\n\n# Combine title and text as input with proper prompt engineering\ntrain_data['input_text'] = \"Classify the hazard category: \" +  train_data['text']\nvalid_data['input_text'] = \"Classify the hazard category: \" + valid_data['title'] + \" \" + valid_data['text']\n\n# Get unique labels and create mapping\nunique_labels = sorted(train_data['hazard-category'].unique())\nlabel_mapping = {label: str(idx) for idx, label in enumerate(unique_labels)}\nlabel_inverse_mapping = {str(idx): label for label, idx in label_mapping.items()}\n\n# Convert labels to text format suitable for T5\ntrain_data['label_text'] = train_data['hazard-category'].map(label_mapping)\nvalid_data['label_text'] = valid_data['hazard-category'].map(label_mapping)\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=256):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = str(self.labels[idx])\n\n        # Encode input text\n        inputs = self.tokenizer.encode_plus(\n            text,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        # Encode label - important to use return_tensors='pt'\n        labels = self.tokenizer.encode_plus(\n            label,\n            max_length=4,  # Short max_length for classification labels\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n            'labels': labels['input_ids'].squeeze()\n        }\n\n# Initialize tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n\n# Prepare datasets\ntrain_dataset = TextClassificationDataset(\n    train_data['input_text'].tolist(),\n    train_data['label_text'].tolist(),\n    tokenizer\n)\n\nvalid_dataset = TextClassificationDataset(\n    valid_data['input_text'].tolist(),\n    valid_data['label_text'].tolist(),\n    tokenizer\n)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=8)\n\n# Setup training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5)\nnum_epochs = 100\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        \n        # Move batch to device\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        # Backward pass\n        loss.backward()\n        optimizer.step()\n    \n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n\n# Evaluation\nmodel.eval()\npredictions = []\ntrue_labels = []\n\nwith torch.no_grad():\n    for batch in valid_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=4,\n            num_beams=4\n        )\n        \n        # Decode predictions\n        decoded_preds = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n        decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n        \n        predictions.extend(decoded_preds)\n        true_labels.extend(decoded_labels)\n\n# Convert predictions and labels back to original categories\npredictions = [label_inverse_mapping[pred] for pred in predictions]\ntrue_labels = [label_inverse_mapping[label] for label in true_labels]\n\n# Calculate metrics\naccuracy = accuracy_score(true_labels, predictions)\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\nmacro_precision = precision_score(true_labels, predictions, average='macro')\nmacro_recall = recall_score(true_labels, predictions, average='macro')\n\nprint(\"\\nFinal Metrics:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Macro F1-Score: {macro_f1:.4f}\")\nprint(f\"Macro Precision: {macro_precision:.4f}\")\nprint(f\"Macro Recall: {macro_recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T18:42:26.178254Z","iopub.execute_input":"2025-01-26T18:42:26.178695Z","iopub.status.idle":"2025-01-26T18:47:16.448445Z","shell.execute_reply.started":"2025-01-26T18:42:26.178657Z","shell.execute_reply":"2025-01-26T18:47:16.447672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport numpy as np\n\n# Load dataset without the 'hazard-category' label\nvalid_data_test = pd.read_csv(\"unlabed_test.csv\", index_col=0)\nvalid_data_test['input_text'] = \"Classify the hazard category: \" + valid_data_test['title'] + \" \" + valid_data_test['text']\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len=256):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n\n        # Encode input text\n        inputs = self.tokenizer.encode_plus(\n            text,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': inputs['input_ids'].squeeze(0),\n            'attention_mask': inputs['attention_mask'].squeeze(0)\n        }\n\n# Initialize tokenizer and model\n# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n\n# Prepare dataset without labels\nvalid_dataset = TextClassificationDataset(\n    valid_data_test['input_text'].tolist(),\n    tokenizer\n)\n\nvalid_loader = DataLoader(valid_dataset, batch_size=8)\n\n# Setup for evaluation\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Evaluation\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for batch in valid_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        # Generate predictions\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=4,\n            num_beams=4\n        )\n\n        # Decode predictions\n        decoded_preds = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n        predictions.extend(decoded_preds)\n\n# Output the predictions\n# valid_data_test['predicted_label'] = predictions\n\n# Save predictions to a CSV file\n# valid_data_test.to_csv(\"/kaggle/working/predictions.csv\", index=False)\npredictions = [label_inverse_mapping[pred] for pred in predictions]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T18:47:16.449451Z","iopub.execute_input":"2025-01-26T18:47:16.449678Z","iopub.status.idle":"2025-01-26T18:47:44.402137Z","shell.execute_reply.started":"2025-01-26T18:47:16.449660Z","shell.execute_reply":"2025-01-26T18:47:44.401240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nindex = list(range(len(predictions)))\n\n# Create a DataFrame with the index and hazard categories\ndf = pd.DataFrame({\n    'index': index,\n    'hazard-category': predictions\n})\n\n# Save the DataFrame to a CSV file\ndf.to_csv(\"augmentation-flant5-hazard.csv\", index=False)\n\nprint(\"CSV file has been created and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T18:47:44.403532Z","iopub.execute_input":"2025-01-26T18:47:44.403814Z","iopub.status.idle":"2025-01-26T18:47:44.418993Z","shell.execute_reply.started":"2025-01-26T18:47:44.403793Z","shell.execute_reply":"2025-01-26T18:47:44.418132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}